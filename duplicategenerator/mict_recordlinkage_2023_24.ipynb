{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "import recordlinkage as rl\n",
    "from numpy.random import choice\n",
    "\n",
    "# Models to use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run once to download to drive initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the patient record dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../test/test5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data['rec_id'].apply(lambda x: 1 if '-dup-' in x else 0))\n",
    "#print(data['rec_id'].apply(lambda x: x.split('-')[0] if '-dup-' in x else np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up record IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>culture</th>\n",
       "      <th>sex</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>national_identifier</th>\n",
       "      <th>blocking_number</th>\n",
       "      <th>state</th>\n",
       "      <th>address_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>audri</td>\n",
       "      <td>hambledon</td>\n",
       "      <td>5.0</td>\n",
       "      <td>burford place</td>\n",
       "      <td>19390120</td>\n",
       "      <td>2511120932</td>\n",
       "      <td>18597484</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>jody</td>\n",
       "      <td>macdougall</td>\n",
       "      <td>80.0</td>\n",
       "      <td>carstensz street</td>\n",
       "      <td>19110201</td>\n",
       "      <td>07703368</td>\n",
       "      <td>73022768</td>\n",
       "      <td>3</td>\n",
       "      <td>sa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>deandrea</td>\n",
       "      <td>jeffers</td>\n",
       "      <td>7.0</td>\n",
       "      <td>deloraine street</td>\n",
       "      <td>19330308</td>\n",
       "      <td>613990563</td>\n",
       "      <td>10639456</td>\n",
       "      <td>9</td>\n",
       "      <td>qld</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>tommie</td>\n",
       "      <td>traves</td>\n",
       "      <td>24.0</td>\n",
       "      <td>wilshire street</td>\n",
       "      <td>19940531</td>\n",
       "      <td>2513050578</td>\n",
       "      <td>27730848</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ferndale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>jeri</td>\n",
       "      <td>edwardson</td>\n",
       "      <td>17.0</td>\n",
       "      <td>plant road</td>\n",
       "      <td>19350506</td>\n",
       "      <td>07645660</td>\n",
       "      <td>24358245</td>\n",
       "      <td>0</td>\n",
       "      <td>sa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rec_id culture sex given_name     surname  street_number         address_1   \n",
       "0      0     eng   f      audri   hambledon            5.0     burford place  \\\n",
       "1      1     eng   f       jody  macdougall           80.0  carstensz street   \n",
       "2      2     eng   f   deandrea     jeffers            7.0  deloraine street   \n",
       "3      3     eng   f     tommie      traves           24.0   wilshire street   \n",
       "4      4     eng   f       jeri   edwardson           17.0        plant road   \n",
       "\n",
       "  date_of_birth phone_number national_identifier  blocking_number state   \n",
       "0      19390120   2511120932            18597484                4   NaN  \\\n",
       "1      19110201     07703368            73022768                3    sa   \n",
       "2      19330308    613990563            10639456                9   qld   \n",
       "3      19940531   2513050578            27730848                5   NaN   \n",
       "4      19350506     07645660            24358245                0    sa   \n",
       "\n",
       "  address_2  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3  ferndale  \n",
       "4       NaN  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup records\n",
    "data['rec_id'] = data['rec_id'].str.replace(\"rec-\", \"\")\n",
    "data['rec_id'] = data['rec_id'].str.replace(\"-org\", \"\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>culture</th>\n",
       "      <th>sex</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>national_identifier</th>\n",
       "      <th>blocking_number</th>\n",
       "      <th>state</th>\n",
       "      <th>address_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>523-dup-0</td>\n",
       "      <td>eng</td>\n",
       "      <td>m</td>\n",
       "      <td>det a</td>\n",
       "      <td>garde nar</td>\n",
       "      <td>7.0</td>\n",
       "      <td>victoria street</td>\n",
       "      <td>19510 211</td>\n",
       "      <td>2775392</td>\n",
       "      <td>17858385</td>\n",
       "      <td>7</td>\n",
       "      <td>nsw</td>\n",
       "      <td>knowsley park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>849-dup-0</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>caitrona</td>\n",
       "      <td>keorgejon</td>\n",
       "      <td>46.0</td>\n",
       "      <td>currong street</td>\n",
       "      <td>19990422</td>\n",
       "      <td>2977950</td>\n",
       "      <td>54672668</td>\n",
       "      <td>7</td>\n",
       "      <td>nsw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>75-dup-0</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>emil  a</td>\n",
       "      <td>mckell ar</td>\n",
       "      <td>47.0</td>\n",
       "      <td>crowder circuit</td>\n",
       "      <td>1904067</td>\n",
       "      <td>2513104854</td>\n",
       "      <td>63767305</td>\n",
       "      <td>8</td>\n",
       "      <td>vic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>131-dup-0</td>\n",
       "      <td>eng</td>\n",
       "      <td>f</td>\n",
       "      <td>atilha</td>\n",
       "      <td>mcnahb</td>\n",
       "      <td>2.0</td>\n",
       "      <td>dobell circuit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36030263</td>\n",
       "      <td>3</td>\n",
       "      <td>nsw</td>\n",
       "      <td>mount sandiman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>755-dup-0</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sezttsica</td>\n",
       "      <td>bramjon</td>\n",
       "      <td>86.0</td>\n",
       "      <td>western hill street</td>\n",
       "      <td>19580316</td>\n",
       "      <td>8227367</td>\n",
       "      <td>30562800</td>\n",
       "      <td>3</td>\n",
       "      <td>vic</td>\n",
       "      <td>rowethorpe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rec_id culture  sex given_name    surname  street_number   \n",
       "1795  523-dup-0     eng    m      det a  garde nar            7.0  \\\n",
       "1796  849-dup-0     eng    f   caitrona  keorgejon           46.0   \n",
       "1797   75-dup-0     eng    f    emil  a  mckell ar           47.0   \n",
       "1798  131-dup-0     eng    f     atilha     mcnahb            2.0   \n",
       "1799  755-dup-0     eng  NaN  sezttsica    bramjon           86.0   \n",
       "\n",
       "                address_1 date_of_birth phone_number national_identifier   \n",
       "1795      victoria street     19510 211      2775392            17858385  \\\n",
       "1796       currong street      19990422      2977950            54672668   \n",
       "1797      crowder circuit       1904067   2513104854            63767305   \n",
       "1798       dobell circuit           NaN          NaN            36030263   \n",
       "1799  western hill street      19580316      8227367            30562800   \n",
       "\n",
       "      blocking_number state       address_2  \n",
       "1795                7   nsw   knowsley park  \n",
       "1796                7   nsw             NaN  \n",
       "1797                8   vic             NaN  \n",
       "1798                3   nsw  mount sandiman  \n",
       "1799                3   vic      rowethorpe  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values and Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'street_number', 'address_1', and 'address_2' into one field and clean it\n",
    "data['address'] = data['street_number'].astype(str) + ' ' + data['address_1'].astype(str) + ' ' + data['address_2'].astype(str)\n",
    "data['address'] = data['address'].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "\n",
    "data['date_of_birth']  = pd.to_datetime(data['date_of_birth'], errors = 'coerce')\n",
    "data['day'] = data['date_of_birth'].dt.strftime('%d')\n",
    "data['month'] = data['date_of_birth'].dt.strftime('%m')\n",
    "data['year'] = data['date_of_birth'].dt.strftime('%Y')\n",
    "\n",
    "# Drop the 'blocking_number' field\n",
    "data = data.drop(columns=['blocking_number', 'street_number', 'address_1', 'address_2', 'date_of_birth', 'culture', 'state'])\n",
    "\n",
    "for col in [\"surname\", \"given_name\", \"address\"]:\n",
    "    data[col] = data[col].fillna('')\n",
    "    data[col] = data[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All fields: ['rec_id', 'sex', 'given_name', 'surname', 'phone_number', 'national_identifier', 'address', 'day', 'month', 'year', 'match', 'match_id']\n"
     ]
    }
   ],
   "source": [
    "# Identify known duplicates based on the rec_id column\n",
    "duplicates = data[data[\"rec_id\"].str.contains(\"-dup-\")]\n",
    "\n",
    "# Create a dictionary mapping each duplicate record to its corresponding original record\n",
    "originals = {}\n",
    "for i, row in duplicates.iterrows():\n",
    "    original_id = row[\"rec_id\"].replace(\"-dup-0\", \"\")\n",
    "    if original_id in originals:\n",
    "        originals[original_id].append(i)\n",
    "    else:\n",
    "        originals[original_id] = [i]\n",
    "\n",
    "# Create a new column called \"match\" to indicate whether a record is a duplicate or not\n",
    "data[\"match\"] = 0\n",
    "data[\"match_id\"] = data[\"rec_id\"]\n",
    "for original_id, duplicates in originals.items():\n",
    "    data.loc[duplicates, \"match\"] = 1\n",
    "    data.loc[duplicates, \"match_id\"] = original_id\n",
    "    # this ensures that both the duplicate and the match are in the same training set\n",
    "    data.loc[data['rec_id'] == original_id, 'match'] = 1\n",
    "    data.loc[data['rec_id'] == original_id, 'match_id'] = original_id\n",
    "\n",
    "data[\"match_id\"] = data[\"match_id\"].astype(int)\n",
    "\n",
    "# get all fields\n",
    "all_fields = data.columns.values.tolist()\n",
    "print(\"All fields:\", all_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos\n",
      "\n",
      "          y_name_leven  y_surname_leven  y_name_jaro  y_surname_jaro\n",
      "1   1493      0.750000             0.80     0.883333        0.893333\n",
      "2   1791      0.875000             1.00     0.950000        1.000000\n",
      "4   1684      1.000000             0.75     1.000000        0.950000\n",
      "5   1326      0.857143             0.40     0.942857        0.600000\n",
      "6   1562      0.500000             1.00     0.875556        1.000000\n",
      "...                ...              ...          ...             ...\n",
      "995 1351      0.750000             0.60     0.925000        0.805000\n",
      "996 1245      0.666667             0.00     0.805556        0.000000\n",
      "997 1184      1.000000             1.00     1.000000        1.000000\n",
      "998 1247      0.571429             0.75     0.771429        0.908333\n",
      "999 1360      1.000000             0.75     1.000000        0.883333\n",
      "\n",
      "[800 rows x 4 columns]\n",
      "Neg\n",
      "\n",
      "           y_name_leven  y_surname_leven  y_name_jaro  y_surname_jaro\n",
      "555  612       0.333333         0.000000     0.555556        0.412037\n",
      "472  1431      0.142857         0.250000     0.373016        0.550000\n",
      "263  1711      0.285714         0.000000     0.561905        0.000000\n",
      "348  641       0.285714         0.142857     0.595238        0.447619\n",
      "1197 1559      0.333333         0.000000     0.458333        0.425926\n",
      "...                 ...              ...          ...             ...\n",
      "695  255       0.000000         0.000000     0.000000        0.000000\n",
      "1766 1227      0.000000         0.111111     0.000000        0.518519\n",
      "392  546       0.000000         0.111111     0.000000        0.458333\n",
      "1619 645       0.000000         0.222222     0.000000        0.476190\n",
      "14   1750      0.125000         0.222222     0.527778        0.611111\n",
      "\n",
      "[800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test true links\n",
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    unique_match_id = unique_match_id[~np.isnan(unique_match_id)] # remove nan values\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return ((f11 == f22) & (f12 == f21)).astype(float)\n",
    "\n",
    "def join_names_space(f11, f12, f21, f22):\n",
    "    return ((f11+\" \"+f12 == f21) | (f11+\" \"+f12 == f22)| (f21+\" \"+f22 == f11)| (f21+\" \"+f22 == f12)).astype(float)\n",
    "\n",
    "def join_names_dash(f11, f12, f21, f22):\n",
    "    return ((f11+\"-\"+f12 == f21) | (f11+\"-\"+f12 == f22)| (f21+\"-\"+f22 == f11)| (f21+\"-\"+f22 == f12)).astype(float)\n",
    "\n",
    "def abb_surname(f1, f2):\n",
    "    return ((f1[0]==f2) | (f1==f2[0])).astype(float)\n",
    "\n",
    "def reset_day(f11, f12, f21, f22):\n",
    "    return (((f11 == 1) & (f12 == 1))|((f21 == 1) & (f22 == 1))).astype(float)\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='levenshtein', label='y_name_leven')\n",
    "    c.string('surname', 'surname', method='levenshtein', label='y_surname_leven')  \n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name_jaro')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname_jaro')  \n",
    "    # c.string('postcode', 'postcode', method='jarowinkler', label='y_postcode')      \n",
    "    # exact_fields = ['postcode', 'address_1', 'address_2', 'street_number']\n",
    "    # for field in exact_fields:\n",
    "    #     c.exact(field, field, label='y_'+field+'_exact')\n",
    "    #c.compare_vectorized(reset_day,('day', 'month'), ('day', 'month'),label='reset_day_flag')    \n",
    "    # c.compare_vectorized(swap_fields_flag,('day', 'month'), ('day', 'month'),label='swap_day_month')    \n",
    "    # c.compare_vectorized(swap_fields_flag,('surname', 'given_name'), ('surname', 'given_name'),label='swap_names')    \n",
    "    # c.compare_vectorized(join_names_space,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_space')\n",
    "    # c.compare_vectorized(join_names_dash,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_dash')\n",
    "    # c.compare_vectorized(abb_surname,'surname', 'surname',label='abb_surname')\n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "# see example of true links\n",
    "true_links_data = generate_true_links(data)\n",
    "#print(true_links_data)\n",
    "\n",
    "false_links_data = generate_false_links(data, len(true_links_data))\n",
    "#print(false_links_data)\n",
    "\n",
    "# get negative and positive features\n",
    "pos = extract_features(data, true_links_data)\n",
    "print('Pos\\n')\n",
    "print(pos)\n",
    "\n",
    "neg = extract_features(data, false_links_data)\n",
    "print('Neg\\n')\n",
    "print(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>national_identifier</th>\n",
       "      <th>address</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>match</th>\n",
       "      <th>match_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>audri</td>\n",
       "      <td>hambledon</td>\n",
       "      <td>2511120932</td>\n",
       "      <td>18597484</td>\n",
       "      <td>5 0 burford place nan</td>\n",
       "      <td>20</td>\n",
       "      <td>01</td>\n",
       "      <td>1939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>jody</td>\n",
       "      <td>macdougall</td>\n",
       "      <td>07703368</td>\n",
       "      <td>73022768</td>\n",
       "      <td>80 0 carstensz street nan</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>1911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>deandrea</td>\n",
       "      <td>jeffers</td>\n",
       "      <td>613990563</td>\n",
       "      <td>10639456</td>\n",
       "      <td>7 0 deloraine street nan</td>\n",
       "      <td>08</td>\n",
       "      <td>03</td>\n",
       "      <td>1933</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>tommie</td>\n",
       "      <td>traves</td>\n",
       "      <td>2513050578</td>\n",
       "      <td>27730848</td>\n",
       "      <td>24 0 wilshire street ferndale</td>\n",
       "      <td>31</td>\n",
       "      <td>05</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>f</td>\n",
       "      <td>jeri</td>\n",
       "      <td>edwardson</td>\n",
       "      <td>07645660</td>\n",
       "      <td>24358245</td>\n",
       "      <td>17 0 plant road nan</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>1935</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rec_id sex given_name     surname phone_number national_identifier   \n",
       "0       0   f      audri   hambledon   2511120932            18597484  \\\n",
       "1       1   f       jody  macdougall     07703368            73022768   \n",
       "2       2   f   deandrea     jeffers    613990563            10639456   \n",
       "3       3   f     tommie      traves   2513050578            27730848   \n",
       "4       4   f       jeri   edwardson     07645660            24358245   \n",
       "\n",
       "                         address day month  year  match  match_id  \n",
       "0          5 0 burford place nan  20    01  1939      0         0  \n",
       "1      80 0 carstensz street nan  01    02  1911      1         1  \n",
       "2       7 0 deloraine street nan  08    03  1933      1         2  \n",
       "3  24 0 wilshire street ferndale  31    05  1994      0         3  \n",
       "4            17 0 plant road nan  06    05  1935      1         4  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Head')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tail\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>national_identifier</th>\n",
       "      <th>address</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>match</th>\n",
       "      <th>match_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>1795</td>\n",
       "      <td>m</td>\n",
       "      <td>det a</td>\n",
       "      <td>garde nar</td>\n",
       "      <td>2775392</td>\n",
       "      <td>17858385</td>\n",
       "      <td>7 0 victoria street knowsley park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>1796</td>\n",
       "      <td>f</td>\n",
       "      <td>caitrona</td>\n",
       "      <td>keorgejon</td>\n",
       "      <td>2977950</td>\n",
       "      <td>54672668</td>\n",
       "      <td>46 0 currong street nan</td>\n",
       "      <td>22</td>\n",
       "      <td>04</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>1797</td>\n",
       "      <td>f</td>\n",
       "      <td>emil  a</td>\n",
       "      <td>mckell ar</td>\n",
       "      <td>2513104854</td>\n",
       "      <td>63767305</td>\n",
       "      <td>47 0 crowder circuit nan</td>\n",
       "      <td>07</td>\n",
       "      <td>06</td>\n",
       "      <td>1904</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>1798</td>\n",
       "      <td>f</td>\n",
       "      <td>atilha</td>\n",
       "      <td>mcnahb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36030263</td>\n",
       "      <td>2 0 dobell circuit mount sandiman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sezttsica</td>\n",
       "      <td>bramjon</td>\n",
       "      <td>8227367</td>\n",
       "      <td>30562800</td>\n",
       "      <td>86 0 western hill street rowethorpe</td>\n",
       "      <td>16</td>\n",
       "      <td>03</td>\n",
       "      <td>1958</td>\n",
       "      <td>1</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rec_id  sex given_name    surname phone_number national_identifier   \n",
       "1795    1795    m      det a  garde nar      2775392            17858385  \\\n",
       "1796    1796    f   caitrona  keorgejon      2977950            54672668   \n",
       "1797    1797    f    emil  a  mckell ar   2513104854            63767305   \n",
       "1798    1798    f     atilha     mcnahb          NaN            36030263   \n",
       "1799    1799  NaN  sezttsica    bramjon      8227367            30562800   \n",
       "\n",
       "                                  address  day month  year  match  match_id  \n",
       "1795    7 0 victoria street knowsley park  NaN   NaN   NaN      1       523  \n",
       "1796              46 0 currong street nan   22    04  1999      1       849  \n",
       "1797             47 0 crowder circuit nan   07    06  1904      1        75  \n",
       "1798    2 0 dobell circuit mount sandiman  NaN   NaN   NaN      1       131  \n",
       "1799  86 0 western hill street rowethorpe   16    03  1958      1       755  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Tail')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the mean embedding for a list of tokens\n",
    "def get_mean_embedding(tokens):\n",
    "    embeddings = [model_w2v.wv[token] for token in tokens if token in model_w2v.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model_w2v.vector_size)  # Return a zero vector if no valid tokens\n",
    "\n",
    "# Remove extra spaces from string columns\n",
    "string_columns = ['given_name', 'surname', 'address']\n",
    "for col in string_columns:\n",
    "    data[col] = data[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Label encoding for 'sex'\n",
    "label_encoder_columns = ['sex']\n",
    "for col in label_encoder_columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[col] = label_encoder.fit_transform(data[col].astype(str).str.lower())\n",
    "\n",
    "# Tokenize the string columns and store them in a new DataFrame\n",
    "tokenized_data = pd.DataFrame()\n",
    "for col in string_columns:\n",
    "    tokenized_data[col] = data[col].apply(word_tokenize)\n",
    "\n",
    "# Generate the Word2Vec embeddings for the tokenized string columns\n",
    "vector_size = 200\n",
    "model_w2v = Word2Vec(tokenized_data.values.flatten(), vector_size=vector_size, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model_w2v.save(\"word2vec_properties.model\")\n",
    "\n",
    "# Define the weights for the string columns\n",
    "weights = {\n",
    "    'given_name': 1, \n",
    "    'surname': 1, \n",
    "    'address': 1, \n",
    "    'phone_number': 1, \n",
    "    'national_identifier': 1,\n",
    "    'sex': 1,\n",
    "    'date_of_birth': 1\n",
    "}\n",
    "\n",
    "# Apply the Word2Vec embeddings to the tokenized string columns\n",
    "# New way gives issues\n",
    "# for col in string_columns:\n",
    "#     data[col] = tokenized_data[col].apply(get_mean_embedding).values\n",
    "\n",
    "# Old Way\n",
    "embedded_data = pd.DataFrame()\n",
    "for col in string_columns:\n",
    "    embeddings = np.vstack(tokenized_data[col].apply(get_mean_embedding).values)\n",
    "    temp_df = pd.DataFrame(embeddings, columns=[f\"{col}_embed_{i}\" for i in range(embeddings.shape[1])])\n",
    "    temp_df *= weights[col]  # Apply the weights\n",
    "    embedded_data = pd.concat([embedded_data, temp_df], axis=1)\n",
    "\n",
    "# Replace the original string columns with the embedded columns\n",
    "data = pd.concat([data.drop(columns=string_columns), embedded_data], axis=1)\n",
    "\n",
    "# Impute missing values for numerical columns with the mean value\n",
    "numerical_columns = ['phone_number', 'national_identifier']\n",
    "for col in numerical_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    mean_value = data[col].mean() \n",
    "    data[col].fillna(value=mean_value, inplace=True) \n",
    "\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# data[numerical_columns] = imputer.fit_transform(data[numerical_columns])\n",
    "\n",
    "# Standardize numerical columns\n",
    "# scaler = StandardScaler()\n",
    "# data[numerical_columns] = scaler.fit_transform(data[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Vector Columns & Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>national_identifier</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>match</th>\n",
       "      <th>match_id</th>\n",
       "      <th>given_name_embed_0</th>\n",
       "      <th>...</th>\n",
       "      <th>address_embed_190</th>\n",
       "      <th>address_embed_191</th>\n",
       "      <th>address_embed_192</th>\n",
       "      <th>address_embed_193</th>\n",
       "      <th>address_embed_194</th>\n",
       "      <th>address_embed_195</th>\n",
       "      <th>address_embed_196</th>\n",
       "      <th>address_embed_197</th>\n",
       "      <th>address_embed_198</th>\n",
       "      <th>address_embed_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.511121e+09</td>\n",
       "      <td>18597484.0</td>\n",
       "      <td>20</td>\n",
       "      <td>01</td>\n",
       "      <td>1939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028127</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>-0.021773</td>\n",
       "      <td>0.027250</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>-0.036931</td>\n",
       "      <td>-0.016302</td>\n",
       "      <td>-0.019821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.703368e+06</td>\n",
       "      <td>73022768.0</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>1911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028563</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>-0.005574</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>-0.037842</td>\n",
       "      <td>-0.016008</td>\n",
       "      <td>-0.019694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.139906e+08</td>\n",
       "      <td>10639456.0</td>\n",
       "      <td>08</td>\n",
       "      <td>03</td>\n",
       "      <td>1933</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-0.004118</td>\n",
       "      <td>-0.024944</td>\n",
       "      <td>0.031909</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>-0.040786</td>\n",
       "      <td>-0.018715</td>\n",
       "      <td>-0.020810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.513051e+09</td>\n",
       "      <td>27730848.0</td>\n",
       "      <td>31</td>\n",
       "      <td>05</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024389</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.002529</td>\n",
       "      <td>-0.019023</td>\n",
       "      <td>0.023348</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>-0.030894</td>\n",
       "      <td>-0.011959</td>\n",
       "      <td>-0.016415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7.645660e+06</td>\n",
       "      <td>24358245.0</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>1935</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.001987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>-0.018493</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>-0.031213</td>\n",
       "      <td>-0.014972</td>\n",
       "      <td>-0.014459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 609 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rec_id  sex  phone_number  national_identifier day month  year  match   \n",
       "0       0    0  2.511121e+09           18597484.0  20    01  1939      0  \\\n",
       "1       1    0  7.703368e+06           73022768.0  01    02  1911      1   \n",
       "2       2    0  6.139906e+08           10639456.0  08    03  1933      1   \n",
       "3       3    0  2.513051e+09           27730848.0  31    05  1994      0   \n",
       "4       4    0  7.645660e+06           24358245.0  06    05  1935      1   \n",
       "\n",
       "   match_id  given_name_embed_0  ...  address_embed_190  address_embed_191   \n",
       "0         0            0.001556  ...           0.028127           0.006696  \\\n",
       "1         1           -0.004509  ...           0.028563           0.006779   \n",
       "2         2            0.001990  ...           0.031771           0.006614   \n",
       "3         3           -0.001892  ...           0.024389           0.004425   \n",
       "4         4           -0.001987  ...           0.023014           0.007070   \n",
       "\n",
       "   address_embed_192  address_embed_193  address_embed_194  address_embed_195   \n",
       "0          -0.004842          -0.021773           0.027250           0.001877  \\\n",
       "1          -0.005574          -0.023863           0.029619           0.002801   \n",
       "2          -0.004118          -0.024944           0.031909           0.002218   \n",
       "3          -0.002529          -0.019023           0.023348           0.002352   \n",
       "4          -0.004024          -0.018493           0.023192           0.003244   \n",
       "\n",
       "   address_embed_196  address_embed_197  address_embed_198  address_embed_199  \n",
       "0           0.005991          -0.036931          -0.016302          -0.019821  \n",
       "1           0.006110          -0.037842          -0.016008          -0.019694  \n",
       "2           0.006340          -0.040786          -0.018715          -0.020810  \n",
       "3           0.004817          -0.030894          -0.011959          -0.016415  \n",
       "4           0.004636          -0.031213          -0.014972          -0.014459  \n",
       "\n",
       "[5 rows x 609 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('surname\\n')\n",
    "# print(data['surname'][0])\n",
    "# print('given_name\\n')\n",
    "# print(data['given_name'][0])\n",
    "# print('address\\n')\n",
    "# print(data['address'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
    "    if modeltype == 'rf': # Random Forest\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=modeltype_2, max_depth=modelparam)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'gbm': # Gradient Boosted Trees\n",
    "        model = GradientBoostingClassifier(n_estimators=100, random_state=42, loss=modeltype_2, learning_rate=modelparam)\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'sc': # StackingClassifier\n",
    "        if modelparam < 2:\n",
    "            modelparam = 2\n",
    "        if modelparam > 1000:\n",
    "            modelparam = 1000\n",
    "        estimators = [\n",
    "            ('svr', make_pipeline(StandardScaler(),\n",
    "                                LinearSVC(random_state=42)))\n",
    "        ]\n",
    "        #model = StackingClassifier(estimators=estimators, stack_method=modeltype_2, cv=modelparam, n_jobs=-1)\n",
    "        model = StackingClassifier(estimators=estimators, cv=modelparam, n_jobs=-1, final_estimator=LogisticRegression())\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'lg': # Logistic Regression\n",
    "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=42)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nn': # Neural Network\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = modeltype_2,random_state=42, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=30000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "def evaluation(test_labels, result):\n",
    "    a = accuracy_score(test_labels, result)\n",
    "    c = confusion_matrix(test_labels, result)\n",
    "    p = precision_score(test_labels, result)\n",
    "    f = f1_score(test_labels, result)\n",
    "    r = recall_score(test_labels, result)\n",
    "    \n",
    "    metrics_result = {\n",
    "        'confusion_matrix': c,\n",
    "        'precision': p,\n",
    "        'F-score': f, \n",
    "        'accuracy_score': a,\n",
    "        'recall_score': r,\n",
    "        'sensitivity': 'not set'\n",
    "    }\n",
    "    return metrics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=['rec_id', 'match'])\n",
    "y = data['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sc , Param_1: predict , tuning range: [100, 200, 500, 1000]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_base.py\", line 46, in _fit_single_estimator\n    estimator.fit(X, y)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_classes.py\", line 263, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\daneh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLinearSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_index], y\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modelparam \u001b[38;5;129;01min\u001b[39;00m modelparam_range:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#print(f\"Training {modeltype[i]} with model param {modelparam}\\nu\")\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     md \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodeltype\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodeltype_2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m classify(md, X_test)\n\u001b[0;32m     27\u001b[0m     final_eval \u001b[38;5;241m=\u001b[39m evaluation(y_test, final_result)\n",
      "Cell \u001b[1;32mIn[159], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(modeltype, modelparam, train_vectors, train_labels, modeltype_2)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#model = StackingClassifier(estimators=estimators, stack_method=modeltype_2, cv=modelparam, n_jobs=-1)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     model \u001b[38;5;241m=\u001b[39m StackingClassifier(estimators\u001b[38;5;241m=\u001b[39mestimators, cv\u001b[38;5;241m=\u001b[39mmodelparam, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression())\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modeltype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression(C\u001b[38;5;241m=\u001b[39mmodelparam, penalty \u001b[38;5;241m=\u001b[39m modeltype_2,class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     22\u001b[0m                                intercept_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     23\u001b[0m                                n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_stacking.py:209\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    216\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Use models\n",
    "modeltype = ['sc', 'nn', 'lg', 'rf', 'gbm'] # choose between 'rf', 'gbm', 'lg', 'sc', 'nn'\n",
    "modeltype_2 = ['predict', 'relu', 'l2', 'gini', 'log_loss']  # 'l2' or 'none' for lg, 'relu' or 'logistic' for nn, 'log_loss', 'deviance', or 'exponential' for gbm, 'gini', 'entropy', or 'log_loss' for rf, \"auto\", 'predict_proba', 'decision_function', 'predict' for sc\n",
    "#modelparam_range = [0.001, 2000, 0.005]\n",
    "modelparam_range = [100,200,500,1000] # C for svm, C for lg, alpha for NN\n",
    "\n",
    "for i in range(len(modeltype)):\n",
    "    print(\"Model:\",modeltype[i],\", Param_1:\",modeltype_2[i], \", tuning range:\", modelparam_range)\n",
    "    precision = []\n",
    "    sensitivity = []\n",
    "    Fscore = []\n",
    "    confusionMatrix = []\n",
    "    accuracyScore = []\n",
    "    recallScore = []\n",
    "    scores = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        #print(f\"Current Test Index: {test_index}\\n\")\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        for modelparam in modelparam_range:\n",
    "            #print(f\"Training {modeltype[i]} with model param {modelparam}\\nu\")\n",
    "            md = train_model(modeltype[i], modelparam, X_train, y_train, modeltype_2[i])\n",
    "            final_result = classify(md, X_test)\n",
    "            final_eval = evaluation(y_test, final_result)\n",
    "            precision += [final_eval['precision']]\n",
    "            sensitivity += [final_eval['sensitivity']]\n",
    "            Fscore += [final_eval['F-score']]\n",
    "            confusionMatrix  += [final_eval['confusion_matrix']]\n",
    "            accuracyScore  += [final_eval['accuracy_score']]\n",
    "            recallScore  += [final_eval['recall_score']]\n",
    "            scores += md.score(X_test, y_test)\n",
    "            #print(f\"Prediction Score {accuracy_score(y_test, final_result)}\\n\")\n",
    "\n",
    "    print(\"Precision:\",precision,\"\\n\")\n",
    "    print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
    "    print(\"F-score:\", Fscore,\"\\n\")\n",
    "    print(\"Accuracy Score:\", accuracyScore,\"\\n\")\n",
    "    print(\"Confusion Matrix:\", confusionMatrix,\"\\n\")\n",
    "    print(\"Recall Score:\", recallScore,\"\\n\")\n",
    "    print(\"Model Scores:\", scores,\"\\n\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the classifiers\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# svm = LinearSVC(random_state=42)\n",
    "# gbm = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Perform Stratified K-Fold Cross-Validation\n",
    "# # when using the boolean of match 0 or 1\n",
    "# #kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# # when using match id numberic values\n",
    "# kfold = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "# rf_accuracies = []\n",
    "# svm_accuracies = []\n",
    "# gbm_accuracies = []\n",
    "\n",
    "# for train_index, test_index in kfold.split(X, y):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#     # Train and evaluate the classifiers\n",
    "#     for clf, acc_list in [(rf, rf_accuracies), (svm, svm_accuracies), (gbm, gbm_accuracies)]:\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         y_pred = clf.predict(X_test)\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         acc_list.append(accuracy)\n",
    "\n",
    "# # Calculate the average accuracy for each classifier\n",
    "# rf_avg_accuracy = np.mean(rf_accuracies)\n",
    "# svm_avg_accuracy = np.mean(svm_accuracies)\n",
    "# gbm_avg_accuracy = np.mean(gbm_accuracies)\n",
    "\n",
    "# print(\"Random Forest Average Accuracy: {:.2f}\".format(rf_avg_accuracy))\n",
    "# print(\"Support Vector Machine Average Accuracy: {:.2f}\".format(svm_avg_accuracy))\n",
    "# print(\"Gradient Boosting Machine Average Accuracy: {:.2f}\".format(gbm_avg_accuracy))\n",
    "\n",
    "# # Select the best model\n",
    "# best_model = None\n",
    "# best_accuracy = 0\n",
    "\n",
    "# for model, accuracy in [(rf, rf_avg_accuracy), (svm, svm_avg_accuracy), (gbm, gbm_avg_accuracy)]:\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_model = model\n",
    "#         best_accuracy = accuracy\n",
    "\n",
    "# print(\"Best Model: {}\".format(type(best_model).__name__))\n",
    "# print(\"Best Model Average Accuracy: {:.2f}\".format(best_accuracy))\n",
    "\n",
    "# # Save the best model\n",
    "# joblib.dump(rf, 'best_rf_model.pkl')\n",
    "# joblib.dump(svm, 'best_svm_model.pkl')\n",
    "# joblib.dump(gbm, 'best_gbm_model.pkl')\n",
    "\n",
    "# # Save the fitted SimpleImputer and StandardScaler instances\n",
    "# #joblib.dump(imputer, 'imputer.pkl')\n",
    "# #joblib.dump(scaler, 'scaler.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
